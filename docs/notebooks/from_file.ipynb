{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using pyPetal From an Input File"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using all modules of pyPetal may take 3 separate calls to different functions, in two different python environments. While this may be inconvenient (or a downright pain) to run normally (as seen in the full pyPetal tutorial), we have implemented tools to make this process more automated than the segmented form we have seen so far."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making the Input File"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pyPetal has functionality to run off of input using a TOML file, instead of inputting arguments through the python functions.\n",
    "\n",
    "Here is an example of a pyPetal input file:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "[inputs]\n",
    "output_dir = 'toml_output/'\n",
    "filenames = [ 'pypetal/pypetal/examples/dat/javelin_continuum.dat',\n",
    "              '/home/stone28/pypetal/pypetal/examples/dat/javelin_yelm.dat',\n",
    "              '/home/stone28/pypetal/pypetal/examples/dat/javelin_zing.dat' ]\n",
    "line_names = ['continuum', 'yelm', 'zing']\n",
    "\n",
    "\n",
    "[params]\n",
    "    [params.general]\n",
    "    plot = false\n",
    "    verbose = true\n",
    "    threads = 40\n",
    "    lag_bounds = [-1000, 1000]\n",
    "    file_fmt = 'ascii'\n",
    "\n",
    "    [params.drw_rej]\n",
    "    run = true\n",
    "    use_for_javelin = true\n",
    "\n",
    "    [params.detrend]\n",
    "    run = true\n",
    "\n",
    "    [params.pyccf]\n",
    "    run = true\n",
    "    nsim = 3500\n",
    "\n",
    "    [params.pyzdcf]\n",
    "    run = true\n",
    "    nsim = 1000\n",
    "    run_plike = true\n",
    "    plike_dir = 'pypetal/plike_v4/'\n",
    "\n",
    "    [params.pyroa]\n",
    "    run = true\n",
    "    nburn = 7000\n",
    "    nchain = 10000\n",
    "    add_var = true\n",
    "    delay_dist = true\n",
    "    subtract_mean = true\n",
    "    psi_types = 'TruncGaussian'\n",
    "\n",
    "    [params.javelin]\n",
    "    run = true\n",
    "    nburn = 50\n",
    "    nchain = 50\n",
    "    nwalker = 100\n",
    "    lagtobaseline = 0.3\n",
    "\n",
    "    [params.weighting]\n",
    "    run = true\n",
    "    k = 3\n",
    "    rel_height = 0.5\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also could have contructed this file from utility functions in pyPetal, given the arguments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_dir = 'pypetal/examples/dat/javelin_'\n",
    "line_names = ['continuum', 'yelm', 'zing']\n",
    "filenames = [ main_dir + x + '.dat' for x in line_names ]\n",
    "\n",
    "output_dir = 'toml_output/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_params = {\n",
    "    'plot': False,\n",
    "    'verbose': True,\n",
    "    'threads': 40,\n",
    "    'lag_bounds': [-1000,1000],\n",
    "    'file_fmt': 'ascii'\n",
    "}\n",
    "\n",
    "run_drw_rej = True\n",
    "drw_rej_params = {\n",
    "    'use_for_javelin': True\n",
    "}\n",
    "\n",
    "run_detrend = False\n",
    "detrend_params = {}\n",
    "\n",
    "run_pyccf = True\n",
    "pyccf_params = {\n",
    "    'nsim': 3500\n",
    "}\n",
    "\n",
    "run_pyzdcf = True\n",
    "pyzdcf_params = {\n",
    "    'nsim': 1000,\n",
    "    'run_plike': True,\n",
    "    'plike_dir': 'pypetal/plike_v4/'\n",
    "}\n",
    "\n",
    "run_pyroa = True\n",
    "pyroa_params = {\n",
    "    'nburn': 7000,\n",
    "    'nchain': 10000,\n",
    "    'add_var': True,\n",
    "    'delay_dist': True,\n",
    "    'subtract_mean': True,\n",
    "    'psi_types': 'TruncGaussian'\n",
    "}\n",
    "\n",
    "run_javelin = True\n",
    "javelin_params = {\n",
    "    'nburn': 50,\n",
    "    'nchain': 50,\n",
    "    'nwalker': 100,\n",
    "    'lagtobaseline': 0.3\n",
    "}\n",
    "\n",
    "run_weighting = True\n",
    "weighting_params = {\n",
    "    'k': 3,\n",
    "    'rel_height': 0.5\n",
    "}\n",
    "\n",
    "\n",
    "run_array = [run_drw_rej, run_detrend, run_pyccf, run_pyzdcf, run_pyroa, \n",
    "             run_javelin, run_weighting]\n",
    "params_array = [general_params, \n",
    "                drw_rej_params, detrend_params, pyccf_params, pyzdcf_params, pyroa_params, \n",
    "                javelin_params, weighting_params]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypetal.fromfile.run_toml import make_toml\n",
    "\n",
    "toml_dict = make_toml(output_dir, filenames, \n",
    "                      run_array, params_array,\n",
    "                      line_names=line_names,\n",
    "                      filename='input.toml')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running from the File"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pyPetal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Performing DRW rejection\n",
      "------------------------\n",
      "jitter: True\n",
      "nsig: 3\n",
      "nwalker: 100\n",
      "nburn: 300\n",
      "nchain: 1000\n",
      "clip: array\n",
      "reject_data: [ True False False]\n",
      "use_for_javelin: True\n",
      "------------------------\n",
      "        \n",
      "\n",
      "Running pyCCF\n",
      "-----------------\n",
      "lag_bounds: [[-1000, 1000], [-1000, 1000]]\n",
      "interp: 2.0000000001\n",
      "nsim: 3500\n",
      "mcmode: 0\n",
      "sigmode: 0.2\n",
      "thres: 0.8\n",
      "nbin: 50\n",
      "-----------------\n",
      "        \n",
      "Failed centroids:  0\n",
      "Failed peaks:  0\n",
      "Failed centroids:  0\n",
      "Failed peaks:  0\n",
      "\n",
      "Running pyZDCF\n",
      "----------------------\n",
      "nsim: 1000\n",
      "minpts: 0\n",
      "uniform_sampling: False\n",
      "omit_zero_lags: True\n",
      "sparse: auto\n",
      "prefix: zdcf\n",
      "run_plike: True\n",
      "plike_dir: pypetal/plike_v4/\n",
      "----------------------\n",
      "        \n",
      "Executing PLIKE\n",
      "Executing PLIKE\n",
      "\n",
      "Running PyROA\n",
      "----------------\n",
      "nburn: 7000\n",
      "nchain: 10000\n",
      "init_tau: [10.0, 10.0]\n",
      "subtract_mean: True\n",
      "div_mean: False\n",
      "add_var: True\n",
      "delay_dist: True\n",
      "psi_types: ['TruncGaussian', 'TruncGaussian']\n",
      "together: True\n",
      "objname: pyroa\n",
      "----------------\n",
      "        \n",
      "Initial Parameter Values\n",
      "     A0            B0    σ0       A1           B1    τ1    Δ1    σ1      A2          B2    τ2    Δ2    σ2    Δ\n",
      "-------  ------------  ----  -------  -----------  ----  ----  ----  ------  ----------  ----  ----  ----  ---\n",
      "2.26171  -6.36307e-16  0.01  1.19302  4.11909e-16    10     1  0.01  0.5882  3.6042e-16    10     1  0.01   10\n",
      "NWalkers=32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [22:24<00:00,  7.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filter: continuum\n",
      "Mean Delay, error: 0.00 (fixed)\n",
      "Filter: yelm\n",
      "Mean Delay, error:  100.82627  (+   1.11104 -   1.07745)\n",
      "Filter: zing\n",
      "Mean Delay, error:  251.27615  (+   0.90225 -   0.83783)\n",
      "\n",
      "\n",
      "Best Fit Parameters\n",
      "     A0           B0        σ0       A1         B1       τ1        Δ1        σ1        A2          B2       τ2        Δ2         σ2        Δ\n",
      "-------  -----------  --------  -------  ---------  -------  --------  --------  --------  ----------  -------  --------  ---------  -------\n",
      "2.23246  -0.00374287  0.318231  1.17476  -0.021073  100.826  0.338319  0.186738  0.581659  0.00868483  251.276  0.344999  0.0855047  11.9584\n"
     ]
    }
   ],
   "source": [
    "from pypetal.fromfile.run_toml import run_from_toml1\n",
    "_ = run_from_toml1('input.toml')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pyPetal-jav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running JAVELIN\n",
      "--------------------\n",
      "rm_type: spec\n",
      "lagtobaseline: 0.3\n",
      "laglimit: [[-1000, 1000], [-1000, 1000]]\n",
      "fixed: True\n",
      "p_fix: True\n",
      "subtract_mean: True\n",
      "nwalker: 100\n",
      "nburn: 50\n",
      "nchain: 50\n",
      "output_chains: True\n",
      "output_burn: True\n",
      "output_logp: True\n",
      "nbin: 50\n",
      "metric: med\n",
      "together: False\n",
      "--------------------\n",
      "        \n",
      "run parallel chains of number 40 \n",
      "start burn-in\n",
      "no priors on sigma and tau\n",
      "penalize lags longer than 0.30 of the baseline\n",
      "nburn: 50 nwalkers: 100 --> number of burn-in iterations: 5000\n",
      "burn-in finished\n",
      "save burn-in chains to /home/stone28/pypetal/toml_output/yelm/javelin/burn_rmap.txt\n",
      "start sampling\n",
      "sampling finished\n",
      "acceptance fractions are\n",
      "0.28 0.02 0.14 0.06 0.02 0.24 0.02 0.32 0.08 0.10 0.02 0.14 0.00 0.00 0.18 0.22 0.22 0.06 0.26 0.12 0.14 0.04 0.08 0.28 0.26 0.04 0.16 0.02 0.06 0.04 0.18 0.06 0.28 0.02 0.00 0.22 0.06 0.12 0.08 0.04 0.24 0.16 0.26 0.08 0.26 0.30 0.14 0.06 0.12 0.10 0.32 0.22 0.32 0.08 0.02 0.12 0.28 0.34 0.04 0.14 0.06 0.04 0.24 0.24 0.04 0.18 0.02 0.02 0.06 0.00 0.02 0.16 0.08 0.12 0.04 0.22 0.06 0.06 0.24 0.12 0.06 0.12 0.26 0.26 0.02 0.22 0.06 0.14 0.20 0.20 0.28 0.16 0.04 0.22 0.04 0.04 0.08 0.12 0.16 0.12\n",
      "save MCMC chains to /home/stone28/pypetal/toml_output/yelm/javelin/chain_rmap.txt\n",
      "save logp of MCMC chains to /home/stone28/pypetal/toml_output/yelm/javelin/logp_rmap.txt\n",
      "HPD of sigma\n",
      "low:    1.676 med    1.676 hig    1.676\n",
      "HPD of tau\n",
      "low:  263.428 med  263.428 hig  263.428\n",
      "HPD of lag_yelm\n",
      "low:   93.040 med  180.863 hig  540.571\n",
      "HPD of wid_yelm\n",
      "low:    0.228 med    0.594 hig    0.934\n",
      "HPD of scale_yelm\n",
      "low:    0.554 med    0.750 hig    0.914\n",
      "covariance matrix calculated\n",
      "covariance matrix decomposed and updated by U\n",
      "run parallel chains of number 40 \n",
      "start burn-in\n",
      "no priors on sigma and tau\n",
      "penalize lags longer than 0.30 of the baseline\n",
      "nburn: 50 nwalkers: 100 --> number of burn-in iterations: 5000\n",
      "burn-in finished\n",
      "save burn-in chains to /home/stone28/pypetal/toml_output/zing/javelin/burn_rmap.txt\n",
      "start sampling\n",
      "sampling finished\n",
      "acceptance fractions are\n",
      "0.08 0.06 0.10 0.04 0.06 0.10 0.02 0.04 0.04 0.08 0.02 0.16 0.06 0.06 0.08 0.00 0.10 0.12 0.08 0.06 0.20 0.12 0.02 0.04 0.08 0.00 0.10 0.04 0.26 0.02 0.04 0.10 0.08 0.16 0.20 0.02 0.06 0.12 0.06 0.10 0.08 0.14 0.00 0.12 0.02 0.02 0.26 0.04 0.08 0.10 0.18 0.02 0.08 0.10 0.12 0.12 0.08 0.04 0.02 0.04 0.06 0.08 0.18 0.06 0.14 0.00 0.12 0.06 0.02 0.00 0.14 0.00 0.04 0.06 0.12 0.16 0.00 0.16 0.02 0.00 0.08 0.06 0.16 0.04 0.02 0.02 0.12 0.18 0.16 0.00 0.04 0.12 0.08 0.00 0.04 0.16 0.06 0.06 0.00 0.04\n",
      "save MCMC chains to /home/stone28/pypetal/toml_output/zing/javelin/chain_rmap.txt\n",
      "save logp of MCMC chains to /home/stone28/pypetal/toml_output/zing/javelin/logp_rmap.txt\n",
      "HPD of sigma\n",
      "low:    1.676 med    1.676 hig    1.676\n",
      "HPD of tau\n",
      "low:  263.428 med  263.428 hig  263.428\n",
      "HPD of lag_zing\n",
      "low: -181.342 med  197.428 hig  550.095\n",
      "HPD of wid_zing\n",
      "low:    0.304 med    0.493 hig    0.770\n",
      "HPD of scale_zing\n",
      "low:    0.482 med    0.621 hig    0.762\n",
      "covariance matrix calculated\n",
      "covariance matrix decomposed and updated by U\n"
     ]
    }
   ],
   "source": [
    "from pypetal_jav.run_toml import run_from_toml_jav\n",
    "_ = run_from_toml_jav('input.toml')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypetal.fromfile.run_toml import run_from_toml2\n",
    "_ = run_from_toml2('input.toml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prior pyPetal run\n",
      "---------------------\n",
      "DRW Rejection: True\n",
      "pyCCF: True\n",
      "pyZDCF: True\n",
      "PyROA: True\n",
      "JAVELIN: True\n",
      "Weighting: True\n",
      "---------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pypetal.utils import load\n",
    "\n",
    "res, summary = load('toml_output/', verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['drw_rej_res', 'pyccf_res', 'pyzdcf_res', 'plike_res', 'pyroa_res', 'javelin_res', 'weighting_res'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['k', 'n0_pyccf', 'peak_bounds_pyccf', 'peak_pyccf', 'lag_pyccf', 'lag_err_pyccf', 'frac_rejected_pyccf', 'n0_javelin', 'peak_bounds_javelin', 'peak_javelin', 'lag_javelin', 'lag_err_javelin', 'frac_rejected_javelin', 'n0_pyroa', 'peak_bounds_pyroa', 'peak_pyroa', 'lag_pyroa', 'lag_err_pyroa', 'frac_rejected_pyroa', 'rmax_pyccf', 'rmax_javelin', 'rmax_pyroa'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary[0].keys()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pypetal_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3e58292519ac2a3193ec94d9e8a1ef69305163db731d60eb7cbc8aea86cb8a51"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
